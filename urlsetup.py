import argparse
import requests
import os
import sys
import random
import time
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# é¢„å®šä¹‰çš„User-Agentåˆ—è¡¨
WINDOWS_UA = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.3',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 11.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/118.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/115.0.1901.200 Safari/537.3',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Brave/120.0.0.0 Chrome/120.0.0.0 Safari/537.3'
]

LINUX_UA = [
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.3',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/118.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.60',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chromium/120.0.6099.71 Safari/537.3',
    'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0'
]

def get_random_ua():
    """éšæœºè¿”å›Windowsæˆ–Linuxçš„User-Agent"""
    return random.choice(WINDOWS_UA + LINUX_UA)

def normalize_url(original_url):
    """è‡ªåŠ¨è¡¥å…¨URLåè®®å¤´å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†"""
    parsed = urlparse(original_url)
    if not parsed.scheme:
        for protocol in ['https://', 'http://']:
            normalized = protocol + original_url
            try:
                # ä½¿ç”¨æ›´å¯é çš„GETæ–¹æ³•æµ‹è¯•
                requests.get(normalized, 
                           timeout=0.3,
                           headers={'User-Agent': get_random_ua()},
                           allow_redirects=False)
                return normalized
            except:
                continue
        return 'http://' + original_url
    return original_url

def check_url(raw_url):
    """å¸¦UAè½®æ¢çš„æ£€æµ‹å‡½æ•°"""
    normalized_url = normalize_url(raw_url)
    last_error = None
    
    for attempt in range(3):
        try:
            current_url = normalized_url.replace('https://', 'http://') if (attempt % 2 == 1) else normalized_url
            response = requests.head(current_url,
                                   timeout=0.3,
                                   allow_redirects=True,
                                   headers={'User-Agent': get_random_ua()})
            return (raw_url, response.url, response.status_code, None)
        except Exception as e:
            last_error = e
            time.sleep(0.1)  # å¢åŠ å°è¯•é—´éš”
    return (raw_url, normalized_url, None, str(last_error))

def main():
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½UAè½®æ¢URLæ£€æµ‹å·¥å…·',
        formatter_class=argparse.RawTextHelpFormatter,
        epilog='''ä½¿ç”¨ç¤ºä¾‹ï¼š
  åŸºæœ¬æ£€æµ‹ï¼š
  python url_checker.py -f urls.txt -o results.csv
  
  é«˜çº§æ¨¡å¼ï¼š
  python url_checker.py -f urls.txt -o results.csv -s 10

åŠŸèƒ½ç‰¹æ€§ï¼š
  âœ” æ¯æ¬¡è¯·æ±‚ä½¿ç”¨éšæœºUAå¤´ï¼ˆ5 Win + 5 Linuxï¼‰
  âœ” è‡ªåŠ¨åè®®è¡¥å…¨å’Œé‡å®šå‘è·Ÿè¸ª
  âœ” ä¸‰é˜¶æ®µæ£€æµ‹æœºåˆ¶ï¼ˆ300msè¶…æ—¶/æ¬¡ï¼‰
  âœ” å®æ—¶ç»“æœæ˜¾ç¤ºå’ŒCSVè¾“å‡º
''')
    parser.add_argument('-f', '--file', required=True, help='URLåˆ—è¡¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰')
    parser.add_argument('-s', '--threads', type=int, default=3, 
                      help='å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š3ï¼Œæœ€å¤§å»ºè®®50ï¼‰')
    args = parser.parse_args()

    if not os.path.exists(args.file):
        print(f"\033[91mé”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{args.file}' ä¸å­˜åœ¨\033[0m")
        sys.exit(1)

    with open(args.file, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]
    
    if not urls:
        print("\033[91mé”™è¯¯ï¼šæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URL\033[0m")
        sys.exit(2)

    print(f"\nğŸ›¡ å¯ç”¨UAè½®æ¢æ¨¡å¼ | çº¿ç¨‹æ•°ï¼š{args.threads} | è¶…æ—¶ï¼š300ms/æ¬¡\n")
    print("å¼€å§‹æ£€æµ‹...\n")

    print_lock = threading.Lock()
    results = []
    processed = 0
    total = len(urls)
    
    def process_result(future):
        nonlocal processed
        with print_lock:
            processed += 1
            raw_url, final_url, status, error = future.result()
            if status:
                results.append(f'"{final_url}",{status}')
                print(f"[{processed}/{total}] \033[94m{raw_url}\033[0m â†’ \033[36m{final_url}\033[0m (\033[1m{status}\033[0m)")
            else:
                print(f"[{processed}/{total}] \033[90m{raw_url}\033[0m â†’ \033[31m{error[:50]}\033[0m")

    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(check_url, url): url for url in urls}
        try:
            for future in as_completed(futures):
                process_result(future)
        except KeyboardInterrupt:
            print("\n\033[91mç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²æ£€æµ‹ç»“æœ...\033[0m")
            executor.shutdown(wait=False, cancel_futures=True)

    # ä¿å­˜ç»“æœ
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write("FINAL_URL,STATUS_CODE\n")
        f.write("\n".join(results))
    
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼æˆåŠŸæ£€æµ‹ï¼š{len(results)}ï¼Œå¤±è´¥ï¼š{len(urls)-len(results)}")
    print(f"   ç»“æœæ–‡ä»¶ä¿å­˜è‡³ï¼š\033[1m{args.output}\033[0m")

if __name__ == "__main__":
    main()